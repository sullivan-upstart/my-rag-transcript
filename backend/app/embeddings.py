# """
# Stubbed embedding layer ‚Äì ZERO-vector placeholder.

# This lets you:
# ‚Ä¢ Upload / chunk transcripts
# ‚Ä¢ Store vectors in Chroma (schema intact)
# ‚Ä¢ Prototype UI, filters, and query plumbing

# Swap back to a real model later by:
#   - uncommenting the SentenceTransformer block
#   - deleting the dummy-vector lines
# """
# from typing import List
# import numpy as np
# import chromadb
# from chromadb.config import Settings

# # ‚îÄ‚îÄ If you‚Äôre ready for real embeddings later, uncomment: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # from sentence_transformers import SentenceTransformer
# # MODEL_PATH = "./models/all-MiniLM-L6-v2"
# # model = SentenceTransformer(MODEL_PATH)
# # VECTOR_SIZE = model.get_sentence_embedding_dimension()
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# VECTOR_SIZE = 384  # MiniLM size; adjust if you choose a different model

# client = chromadb.Client(
#     Settings(persist_directory="./chroma_db", anonymized_telemetry=False)
# )
# collection = client.get_or_create_collection("transcripts")


# # ‚îÄ‚îÄ‚îÄ utility: simple sliding-window splitter ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def _chunk_text(text: str, size: int = 1000, overlap: int = 200) -> List[str]:
#     chunks, i = [], 0
#     while i < len(text):
#         chunks.append(text[i : i + size])
#         i += size - overlap
#     return chunks


# # ‚îÄ‚îÄ‚îÄ public API used by main.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def embed_transcript(text: str, metadata: dict | None = None) -> None:
#     """
#     ‚Ä¢ Splits transcript
#     ‚Ä¢ Stores ZERO vectors (shape = VECTOR_SIZE) in Chroma
#     """
#     metadata = metadata or {}
#     chunks = _chunk_text(text)
#     dummy_vecs = np.zeros((len(chunks), VECTOR_SIZE)).tolist()

#     ids = [f"{metadata.get('filename', 'file')}_{i}" for i in range(len(chunks))]
#     collection.add(
#         ids=ids,
#         documents=chunks,
#         embeddings=dummy_vecs,
#         metadatas=[metadata] * len(chunks),
#     )


# def query_transcripts(question: str, k: int = 4) -> str:
#     """
#     Returns a stub answer until real embeddings + LLM are wired up.
#     You can still test UI‚Üí/query endpoint flow.
#     """
#     return (
#         "[stub] Embedding model not integrated yet.\n"
#         "Once ready, this function will retrieve relevant chunks "
#         "from Chroma and call an LLM to generate an answer."
#     )


"""
Hash-TF-IDF placeholder ¬∑ keeps Chroma happy until real embeddings arrive.
Swap-in later: comment out the vectorizer + hash lines and enable SentenceTransformer.
"""
from typing import List

import numpy as np
from sklearn.feature_extraction.text import HashingVectorizer
import chromadb
from chromadb.config import Settings

# ‚îÄ‚îÄ real model (future) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# from sentence_transformers import SentenceTransformer
# MODEL_PATH = "./models/all-MiniLM-L6-v2"
# model = SentenceTransformer(MODEL_PATH)
# VECTOR_SIZE = model.get_sentence_embedding_dimension()

# ‚îÄ‚îÄ TEMP: hashing-tfidf placeholder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Pick a power of 2 (8k / 16k / 32k). 16 384 ‚âà MiniLM√ó43 in size but compressible.
VECTOR_SIZE = 16_384
_vectorizer = HashingVectorizer(
    n_features=VECTOR_SIZE,
    norm="l2",
    alternate_sign=False,   # keep values non-negative
    stop_words="english",
)

# ‚îÄ‚îÄ Chroma client ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
client = chromadb.Client(
    Settings(persist_directory="./chroma_db", anonymized_telemetry=False)
)
collection = client.get_or_create_collection("transcripts")


def _chunk(text: str, size: int = 1000, overlap: int = 200) -> List[str]:
    out, i = [], 0
    while i < len(text):
        out.append(text[i : i + size])
        i += size - overlap
    return out


def embed_transcript(text: str, metadata: dict | None = None) -> None:
    """
    ‚Ä¢ Split transcript
    ‚Ä¢ Produce Hash-TF-IDF vectors
    ‚Ä¢ Store in Chroma
    """
    metadata = metadata or {}
    chunks = _chunk(text)
    # HashingVectorizer expects an *iterable of str*
    mat = _vectorizer.transform(chunks)        # sparse matrix
    vecs = mat.toarray().tolist()              # dense lists (Chroma wants list[list])
    ids = [f"{metadata.get('filename','file')}_{i}" for i in range(len(chunks))]

    collection.add(
        ids=ids,
        documents=chunks,
        embeddings=vecs,
        metadatas=[metadata] * len(chunks),
    )


def query_transcripts(question: str, k: int = 4) -> str:
    """
    Retrieve with same vectorizer, then **for now** just echo the chunks.
    Later: feed `context` + `question` to an LLM.
    """
    q_vec = _vectorizer.transform([question]).toarray().tolist()
    res = collection.query(query_embeddings=q_vec, n_results=k)
    docs = res["documents"][0]
    context = "\n---\n".join(docs)
    return (
        "üîç **Hash-TF-IDF placeholder answer**\n\n"
        "**Retrieved context:**\n"
        f"{context}"
    )
